# LMB Filter Benchmarks

Performance benchmarking infrastructure for comparing MATLAB, Python, and Rust implementations of LMB filters.

## Quick Start

### Run All Benchmarks (MATLAB + Python)
```bash
./benchmarks/run_all_benchmarks.sh
```

This will:
1. Run MATLAB benchmarks (~2-5 minutes)
2. Run Python benchmarks (~5-15 minutes)
3. Consolidate results
4. Generate comparison reports

Results saved to: `benchmarks/results/`

### Run Python-Only Benchmarks (Faster)
```bash
./benchmarks/run_quick_benchmark.sh [timeout_seconds]
```

Default timeout: 10 seconds per benchmark

Example with 5-second timeout:
```bash
./benchmarks/run_quick_benchmark.sh 5
```

---

## Manual Benchmark Execution

### Python Benchmarks

Run all filters on all scenarios:
```bash
uv run python benchmarks/run_python.py
```

Filter by scenario:
```bash
uv run python benchmarks/run_python.py --scenario n5
```

Filter by filter type:
```bash
uv run python benchmarks/run_python.py --filter LMB
```

Filter by associator:
```bash
uv run python benchmarks/run_python.py --assoc LBP
```

Set custom timeout (default: 10s):
```bash
uv run python benchmarks/run_python.py --timeout 5
```

Combine filters:
```bash
uv run python benchmarks/run_python.py --scenario n10 --filter GA-LMB --assoc LBP
```

### MATLAB Benchmarks

```bash
cd ../multisensor-lmb-filters
octave --eval "addpath('benchmarks'); run_benchmarks"
```

Note: MATLAB only supports single-sensor LMB filters with LBP/Gibbs/Murty.

### Rust Benchmarks (Slow - Not Recommended)

```bash
cargo bench --bench scenario_benchmark
```

⚠️ **Warning:** Criterion benchmarks are very thorough but extremely slow (hours).
Only recommended for detailed performance profiling of specific scenarios.

---

## Benchmark Infrastructure

### Scenarios

Located in `benchmarks/scenarios/*.json`:

- **bouncing_n5_s1.json** - 5 objects, 1 sensor
- **bouncing_n5_s2.json** - 5 objects, 2 sensors
- **bouncing_n10_s1.json** - 10 objects, 1 sensor
- **bouncing_n10_s2.json** - 10 objects, 2 sensors
- **bouncing_n10_s4.json** - 10 objects, 4 sensors
- **bouncing_n20_s1.json** - 20 objects, 1 sensor
- **bouncing_n20_s2.json** - 20 objects, 2 sensors
- **bouncing_n20_s4.json** - 20 objects, 4 sensors
- **bouncing_n20_s8.json** - 20 objects, 8 sensors
- **bouncing_n50_s8.json** - 50 objects, 8 sensors

Scenarios are automatically ordered by complexity (n5 < n10 < n20 < n50).

### Filter Types

**Single-Sensor:**
- LMB - Basic LMB filter
- LMBM - LMB with multi-Bernoulli tracking

**Multi-Sensor:**
- AA-LMB - Arithmetic Average fusion
- GA-LMB - Geometric Average fusion
- PU-LMB - Parallel Update fusion
- IC-LMB - Iterated Corrector fusion
- MS-LMBM - Multi-sensor LMBM

**Associators:**
- LBP - Loopy Belief Propagation (fast, approximate)
- Gibbs - Gibbs sampling (slower, stochastic)
- Murty - Murty's algorithm (slow, exact in Python)

### Early Termination

Once a filter times out on a scenario, all harder scenarios are automatically skipped for that filter. This saves hours of benchmark time.

Example:
```
bouncing_n5_s1 | LMBM-Gibbs | TIMEOUT
bouncing_n5_s2 | LMBM-Gibbs | SKIP     ← Automatically skipped
bouncing_n10_s1 | LMBM-Gibbs | SKIP    ← Automatically skipped
...
```

---

## Output Files

All results saved to `benchmarks/results/`:

### Main Reports
- **BENCHMARK_REPORT.md** - Comprehensive analysis with insights and recommendations
- **QUICK_SUMMARY.txt** - Quick stats and highlights (generated by run_all_benchmarks.sh)
- **comparison_summary.md** - Full MATLAB vs Python vs Rust table

### Data Files
- **all_results.csv** - Raw benchmark data (scenario, filter, impl, time, ospa, status)
- **comparison_data.json** - Structured JSON for programmatic access
- **python_benchmarks.txt** - Raw Python output
- **matlab_benchmarks.txt** - Raw MATLAB output (cleaned)

---

## Consolidation Script

Manually consolidate results from multiple sources:

```bash
uv run python benchmarks/consolidate_results.py \
  --python benchmarks/results/python_benchmarks.txt \
  --matlab benchmarks/results/matlab_benchmarks.txt \
  --output benchmarks/results/
```

Optional: Include Rust Criterion results:
```bash
uv run python benchmarks/consolidate_results.py \
  --criterion target/criterion \
  --python benchmarks/results/python_benchmarks.txt \
  --matlab benchmarks/results/matlab_benchmarks.txt \
  --output benchmarks/results/
```

---

## Understanding Results

### Status Values
- **OK** - Benchmark completed successfully
- **TIMEOUT** - Exceeded time limit (default: 10s)
- **ERROR** - Runtime error or crash
- **SKIP** - Automatically skipped due to earlier timeout
- **N/A** - Not available (e.g., MATLAB doesn't support multi-sensor)

### Performance Metrics
- **Time (ms)** - Execution time in milliseconds
- **OSPA** - Optimal Subpattern Assignment accuracy (Python only)
- **Speedup** - Ratio of baseline/implementation time (>1 = faster, <1 = slower)

### Example Interpretation

```
| bouncing_n5_s1 | LMB-LBP | 20.9 | N/A | 52.9 | - | 0.40× |
```

- MATLAB: 20.9ms
- Rust: Not available (skipped)
- Python: 52.9ms
- Python is **0.40× the speed** of MATLAB (i.e., 2.5× slower)

---

## Common Issues

### Python Benchmarks Timeout on Simple Scenarios

**Problem:** LMBM filters timeout even on n5_s1

**Solution:** This is expected - LMBM is computationally intensive. Results will be automatically skipped for harder scenarios.

### MATLAB Benchmarks Show All ERROR

**Problem:** Multi-sensor scenarios error in MATLAB

**Solution:** This is expected - MATLAB reference implementation only supports single-sensor LMB. These are marked as ERROR and excluded from comparisons.

### Rust Benchmarks Take Hours

**Problem:** Criterion runs extensive statistical analysis

**Solution:**
- For quick timing, use Python benchmarks only
- For detailed Rust profiling, use `cargo bench` with specific filters:
  ```bash
  cargo bench --bench scenario_benchmark -- "LMB-LBP/bouncing_n5_s1"
  ```

---

## Customization

### Adding New Scenarios

1. Create JSON file in `benchmarks/scenarios/`
2. Follow existing format with measurements and ground truth
3. Scenario naming convention: `{scenario_name}_n{objects}_s{sensors}.json`

### Adding New Filters

**Python:**
1. Add filter to `CONFIGS` list in `benchmarks/run_python.py`
2. Format: `(name, filter_class, associator_factory, is_multi_sensor)`

**Rust:**
1. Add benchmark macro call in `benches/scenario_benchmark.rs`
2. Follow existing `bench_lmb!`, `bench_multi!` patterns

**MATLAB:**
1. Modify `benchmarks/run_benchmarks.m` configs array
2. Note: Multi-sensor not supported in reference implementation

---

## Development

### Running Tests First

Before benchmarking, ensure tests pass:

```bash
# Python tests
uv run pytest benchmarks/test_matlab_fixtures.py -v

# Rust tests
cargo test --release

# MATLAB tests
cd ../multisensor-lmb-filters
octave --eval "addpath('fixtures'); run_all_tests"
```

### Debugging Benchmark Failures

Enable verbose output:
```bash
uv run python benchmarks/run_python.py --scenario n5_s1 --filter LMB --assoc LBP
```

Check intermediate values by modifying `run_python.py` to add print statements in the `run_filter` function.

---

## Citation

If you use these benchmarks in research, please cite:

```bibtex
@software{multisensor_lmb_filters_rs,
  title = {Multi-Sensor LMB Filters in Rust},
  author = {Your Name},
  year = {2024},
  url = {https://github.com/yourusername/multisensor-lmb-filters-rs}
}
```

---

## License

See main repository LICENSE file.
